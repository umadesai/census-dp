{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ad Hoc Noise Infusion:** A heuristic label for a very broad class of techniques with no hard rules about their operation. Often ad hoc noise infusion involves additive or multiplicative infusion of noise drawn from a simple closed-form distribution into the attributes of microdata records or aggregates/tabulations. The primary property distinguishing these techniques is negative: they do not come with any form of rigorous guarantee about the protection oﬀered (even against narrow classes of attackers, with some exceptions). <sup>1</sup>\n",
    "\n",
    "**Cell Suppression:** In tabular summaries, a set of “primary suppression” cells is heuristically identiﬁed as sensitive and suppressed. A “secondary suppression” set of cells with known relationships to the primary suppressions are then further suppressed. The goal is to minimize severity of suppression while stopping an attacker who only has access to the published tables from precisely inferring values of the primary suppressions. <sup>1</sup>\n",
    "\n",
    "**Complex Mechanisms:** Highly sophisticated diﬀerential privacy mechanisms almost always use a core set of basic mechanisms as building blocks. <sup>1</sup>\n",
    "\n",
    "**Composition Theorems:** These theorems state that the composition of two differentially private analyses results in a privacy loss that is bounded by the sum of the privacy losses of each of the analyses. <sup>1</sup>\n",
    "\n",
    "**Controlled Tabular Adjustment:** A close cousin of cell suppression, in which cells ﬂagged as sensitive in published tables are protected by systematically (often determinstically) perturbing them, then adjusting the results in a manner controlled to ensure the table’s interior cells still sum to its marginals. The primary distinction between controlled tabular adjustment and cell suppression is that tables published under controlled tabular adjustment are “complete” (no values missing due to suppression). <sup>1</sup>\n",
    "\n",
    "**Data Curator:** The organization that collects and retains the data to be protected, often in a formal database. <sup>1</sup>\n",
    "\n",
    "**Differential Privacy Guarantee:** Differential privacy mathematically guarantees that anyone viewing the result of a\n",
    "differentially private analysis will essentially make the same inference about any individual’s private information, whether or not that individual’s private information is included in the input to the analysis. <sup>2</sup>\n",
    "\n",
    "**Epsilon (ε):** One can think of the parameter ε as determining the overall privacy protection provided by a differentially private analysis. Intuitively, ε determines “how much” of an individual’s privacy an analysis may utilize, or, alternatively, by how much the risk to an individual’s privacy can increase. A smaller value for ε implies better protection (i.e., less risk to privacy). Conversely, a larger value for ε implies worse protection (i.e., higher potential risk to privacy). In particular, ε = 0 implies perfect privacy (i.e., the analysis does not increase any individual’s privacy risk at all).\n",
    "Unfortunately, analyses that satisfy differential privacy with ε = 0 must completely ignore their input data and therefore are useless. <sup>2</sup>\n",
    "\n",
    "**Exponential Mechanism:** Is helpful when dealing with output spaces that are not subsets of Rn (e.g., some mechanisms may have outputs that are cat emojis, or 2-D images). <sup>1</sup>\n",
    "\n",
    "**[Geometric Mechanism:](https://github.com/umadesai/census-dp/blob/master/census_dp/geometric.py)** Is a discrete variant of the Laplace Mechanism. It is useful when integer-valued output is desired. <sup>1</sup>\n",
    "\n",
    "**K-anonymity:** a relatively recent family of techniques primarily targeted at microdata releases. Their deﬁning property is that the frequency of each type of published records must be at least k−1, so that the rarity or uniqueness of records is parametrically controlled. <sup>1</sup>\n",
    "\n",
    "**[Laplace Mechanism:](https://github.com/umadesai/census-dp/blob/master/census_dp/laplace.py)** The Laplace mechanism adds Laplace-distributed noise to a function.\n",
    "\n",
    "**[L1 norm:](https://github.com/umadesai/census-dp/blob/master/census_dp/l1_norm.py)** Is basically minimizing the sum of the absolute differences between the target value and the estimated values. This is a coarse measure: a disclosure limited product with a high L1 compared to the same product without disclosure limitation may still be very accurate for its intended use. \n",
    "\n",
    "**[Matrix Mechanism:](https://github.com/umadesai/census-dp/blob/master/census_dp/matrix_mechanism.py)** Follows a “Select - Measure - Reconstruct” paradigm: given a set of target queries on which low accuracy is desired, this mechanism Selects an alternative set of queries, Measures estimated values of these alternative queries using the Laplace mechanism, and performs post-processing to optimally Reconstruct estimates to the original, target queries. As is common (though certainly not universal) with complex DP mechanisms, the privacy guarantee for MM is simple to analyze, as it relies only on the Laplace mechanism and post-processing. <sup>1</sup>\n",
    "\n",
    "**Multiplicative-Weights Exponential Mechanism** Uses a combination of two basic mechanisms: the exponential mechanism and the Laplace mechanism. These two basic mechanisms are used to iteratively improve on an initial guess at a histogram, focusing progress on queries for which answers appear to be worst. The privacy claim follows from a combination of composition and post-processing coupled with the guarantees of the basic mechanisms. <sup>1</sup>\n",
    "\n",
    "**[NoisyMax:](https://github.com/umadesai/census-dp/blob/master/census_dp/noisy_max.py)** Can be useful when the argmax of a set of queries is needed, but directly estimating all of the queries’ values separately has high global sensitivity. <sup>1</sup>\n",
    "\n",
    "**Parallel Composition:** Is a more eﬃcient but restrictive alternative to sequential composition: when you can invoke parallel composition, you should do so, but you won’t always have that option. We focus here on unbounded DP, but brieﬂy describe parallel composition for bounded DP at this section’s end. <sup>1</sup>\n",
    "\n",
    "**Post-Processing:** One desirable privacy property that diﬀerential privacy possesses is robustness to post-processing. Colloquially, the post-processing property says you can’t outsmart a diﬀerentially private algorithm. An attacker can’t think of a clever way to use the output of a diﬀerentially private mechanism that somehow degrades its deﬁned privacy guarantee. As a result of its immunity to post-processnig, diﬀerential privacy is future-proof in the sense that the guarantees hold regardless of what external information may become available ex post. <sup>1</sup>\n",
    "\n",
    "**Privacy-Accuracy Tradeoff:** There is an inherent trade-oﬀ between privacy and data utility/accuracy discussed by many economists and statisticians. Broadly, the more information that is released (accurately) and with greater granularity, the better the data utility. However, at the same time this increases the risk of an attacker violating a person’s privacy <sup>1</sup>\n",
    "\n",
    "**[Privacy Budget:](https://github.com/umadesai/census-dp/blob/master/notebooks/dp-budget.ipynb)** The privacy loss parameter ε can be thought of as a “privacy budget” to be spent by different analyses of individuals’ data. If a single analysis is expected to be performed on a given set of data, then one might allow this analysis to exhaust the entire privacy budget ε. However, a more typical scenario is that several analyses are expected to be run on a dataset, and, therefore, one needs to calculate the total utilization of the privacy budget by these analyses. <sup>1</sup>\n",
    "\n",
    "**Privacy Loss Parameter:** What can be learned about an individual as a result of her private information being included in a differentially private analysis is limited and quantified by a privacy loss parameter, usually denoted epsilon (ε). Privacy loss can grow as an individual’s information is used in multiple analyses, but the increase is bounded as a known function of ε and the number of\n",
    "analyses performed. <sup>2</sup>\n",
    "\n",
    "**Sequential Composition:** Says that if a mechanism M1 has privacy loss ε1 and another mechanism M2 has privacy loss ε2, then the total privacy loss of the two mechanisms is no more than ε1 + ε2. <sup>1</sup>\n",
    "\n",
    "**Sparse Vector:** Is helpful when estimating a set of queries—most of which are strongly suspected a priori to have small true value—and which have large global sensitivity to estimate directly with the Laplace mechanism, similar to NoisyMax <sup>1</sup>\n",
    "\n",
    "**Standard Mechanisms:** The most basic diﬀerential privacy mechanisms are designed to answer linear queries. They perturb the query output by adding noise drawn from a distribution centered at zero with scale calibrated to the sensitivity of the query and the privacy-loss parameter epsilon <sup>1</sup>\n",
    "\n",
    "**Swapping:** Pairs of “similar” records are randomly paired and swapped. Probabilities, details of similarity calculation, etc are secret <sup>1</sup>\n",
    "\n",
    "**Synthetic Data:** A very broad class of techniques with few rules about the algorithms used, except that they all generate microdata. “Partially” synthetic methods may directly publish unmodiﬁed portions of initial input records, while “fully” synthetic approaches instead make statistical inferences to generate every value in every record. <sup>1</sup>\n",
    "\n",
    "**Top-coding:** Scalar observations (e.g. personal income) exceeding some threshold are perturbed or suppressed. <sup>1</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<sup>1</sup> *An Introduction to Differential Privacy,* 2019 Joint Statistical Meetings, Ashmead, Leclerc, Sexton.\n",
    "\n",
    "<sup>2</sup> *Differential Privacy: A Primer for a Non-Technical Audience*, Wood, Altman, Bembenek, Bun, Gaboardi, Honaker, Nissim, O’Brien, Steinke & Vadhan."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
